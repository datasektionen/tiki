# Docker-compose file for setting up dependencies for a fully working development environment
# This file is used to start up the following services:
# - Postgres
# - Minio (S3-compatible object storage)
# - Imgproxy (for image resizing/proxying)
# - The Phoenix application

services:
  db:
    image: postgres:14
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    volumes:
      - _postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  minio:
    image: minio/minio
    environment:
      MINIO_DOMAIN: localhost # Needed to enable virtual host style requests
      # Encryption stuff
      MINIO_KMS_KES_ENDPOINT: https://play.min.io:7373
      MINIO_KMS_KES_KEY_FILE: root.key
      MINIO_KMS_KES_CERT_FILE: root.cert
      MINIO_KMS_KES_KEY_NAME: my-minio-sse-kms-key
      # Root admin user
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: adminadmin
    # We need to download the root.key and root.cert files before starting the server
    entrypoint: >
      bin/sh -c 'curl -sSL --tlsv1.2 -O "https://raw.githubusercontent.com/minio/kes/master/root.key" -O "https://raw.githubusercontent.com/minio/kes/master/root.cert";
      minio server /data --console-address ":9001";'
    volumes:
      - _minio:/data
    ports:
      - "9000:9000"
      - "9001:9001" # Console port
    healthcheck:
      test: "mc ready local"
      interval: 2s
      timeout: 10s
      retries: 5

  # Service to create buckets and set up encryption on Minio
  createbuckets:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 admin adminadmin;
      mc admin user add myminio IPGAGmgxUBwvZoHowCez RNHuldJLrZSB0RxmpCEYzLOwN0HRIz9RnC2I4qQG;
      mc admin policy attach myminio readwrite --user IPGAGmgxUBwvZoHowCez;
      mc mb myminio/tiki-dev;
      mc encrypt set sse-s3 myminio/tiki-dev;
      exit 0;
      "

  imgproxy:
    image: darthsim/imgproxy
    environment:
      # Imgproxy encryption stuff
      IMGPROXY_KEY: "aaaa"
      IMGPROXY_SALT: "1b1b"
      # Minio configuration
      IMGPROXY_USE_S3: true
      AWS_ACCESS_KEY_ID: IPGAGmgxUBwvZoHowCez
      AWS_SECRET_ACCESS_KEY: RNHuldJLrZSB0RxmpCEYzLOwN0HRIz9RnC2I4qQG
      IMGPROXY_S3_ENDPOINT: "http://minio:9000"
      IMGPROXY_S3_REGION: "eu-north-1"
      IMGPROXY_BASE_URL: "s3://tiki-dev/"
      # Imgproxy configuration
      IMGPROXY_TTL: 31536000
      IMGPROXY_MAX_SRC_RESOLUTION: 30
      IMGPROXY_DEVELOPMENT_ERRORS_MODE: true
    ports:
      - "8080:8080"

  nyckeln:
    image: ghcr.io/datasektionen/nyckeln-under-dorrmattan:latest
    configs:
      - source: nyckeln.yaml
        target: /config.yaml
    ports:
      - 7003:7003 # sso
      - 7004:7004 # hive

  app:
    build:
      context: .
      dockerfile: Dockerfile
    command:
      - "/app/entrypoint.sh"
    env_file:
      - config/.env
    environment:
      # Database stuff
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: tiki_dev
      POSTGRES_HOST: db
      # Imgproxy stuff
      IMGPROXY_KEY: "aaaa"
      IMGPROXY_SALT: "1b1b"
      IMAGE_FRONTEND_URL: "http://localhost:8080"
      # Minio/S3 stuff. In prod, this should be a different IAM user than the one used for imgproxy
      S3_BUCKET_NAME: tiki-dev
      AWS_REGION: eu-north-1
      AWS_ENDPOINT_URL_S3: "http://minio:9000"
      AWS_FRONTEND_ENDPOINT_URL_S3: "http://localhost:9000"
      AWS_ACCESS_KEY_ID: IPGAGmgxUBwvZoHowCez
      AWS_SECRET_ACCESS_KEY: RNHuldJLrZSB0RxmpCEYzLOwN0HRIz9RnC2I4qQG

      # OIDC login stuff
      OIDC_CLIENT_ID: "client-id"
      OIDC_CLIENT_SECRET: "client-secret"
      OIDC_ISSUER_URL: "http://localhost:7003"

      # Hive stuff, see permissions below
      HIVE_URL: "http://nyckeln:7004/api/v1"
      HIVE_API_TOKEN: "totally-a-real-token"
    ports:
      - "4000:4000"
    configs:
      - source: nginx.conf
        target: /etc/nginx/nginx.conf
    volumes:
      - .:/app
      - _elixir_build:/app/_build # So that we can have separate build files for the host and the container
    depends_on:
      - db

volumes:
  _postgres:
  _elixir_build:
  _minio:

configs:
  nyckeln.yaml:
    content: |
      clients:
        - id: "client-id"
          secret: "client-secret"
          redirect_uris:
            - "http://localhost:4000/oidcc/callback"
            - "http://localhost:4000/oidcc/authorize"

      users:
        - kth_id: turetek
          email: turetek@kth.se
          first_name: Ture
          family_name: Teknolog
        - kth_id: money
          email: money@kth.se
          first_name: Lucas
          family_name: Kassör

      hive:
        groups:
          - id: tiki
            members:
              - turetek
            permissions:
              - id: admin
          - id: kassör
            members:
              - money
            permissions:
              - id: audit

  # This is very cursed, but we proxy localhost:7003 to the nyckeln service, so
  # that we can use the same http://localhost:7003 url for the oidc provider worker
  # and the browser when logging in via oidc.
  nginx.conf:
    content: |
      events {}

      http {
        server {
          listen 7003;

          location / {
            proxy_pass http://nyckeln:7003;
          }
        }
      }
